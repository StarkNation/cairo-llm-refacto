{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "hub_name = \"StarkWizard/Mistral-7b-instruct-cairo-instruct\"\n",
    "quant_name = hub_name + \"-AWQ\"\n",
    "\n",
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "# The instruction dataset to use\n",
    "dataset_name = \"StarkWizard/cairo-instruct\"\n",
    "\n",
    "quant_config = {\n",
    "    \"zero_point\":True,\n",
    "    \"q_group_size\":128,\n",
    "    \"w_bit\":4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Model and tokenizer\n",
    "\n",
    "model = AutoAWQForCausalLM.from_pretrained(hub_name,device_map=\"auto\",)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "AWQ:  59%|█████▉    | 19/32 [1:11:10<48:54, 225.75s/it]"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(hub_name,trust_remote_code=True, use_auth_token=True, add_eos_token=True, use_fast=False)\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.pad_token_id = 18610\n",
    "\n",
    "#quantize\n",
    "model.quantize(tokenizer=tokenizer,quant_config=quant_config)\n",
    "\n",
    "#Now we save\n",
    "model.save_quantized(quant_name, safetensors=True, shard_size=\"1GB\")\n",
    "tokenizer.save_pretrained(quant_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.push_to_hub(quant_name)\n",
    "model.push_to_hub(quant_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pushing Files to HF\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "repo = quant_name\n",
    "local = \"./\"+repo+\"/\"\n",
    "files =[\n",
    "    local+\"config.json\",\n",
    "    local+\"generation_config.json\",\n",
    "    local+\"quant_config.json\",\n",
    "    local+\"tokenizer_config.json\",\n",
    "    local+\"special_tokens_map.json\",\n",
    "    local+\"tokenizer.json\",\n",
    "] \n",
    "\n",
    "for f in files:\n",
    "    name = f.split(\"/\")[-1]\n",
    "\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=f,\n",
    "        path_in_repo=name,\n",
    "        repo_id=repo,\n",
    "        repo_type=\"model\",\n",
    "    )\n",
    "    print(f\"Uploaded {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= AutoAWQForCausalLM.from_quantized(quant_name,device_map=\"auto\",fuse_layers=True,\n",
    "                trust_remote_code=True,safetensors=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(quant_name,trust_remote_code=True, use_auth_token=True, add_eos_token=True, use_fast=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
