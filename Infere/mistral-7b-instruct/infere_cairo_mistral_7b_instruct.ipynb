{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "hub_name = \"StarkWizard/Mistral-7b-instruct-cairo-instruct\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model from hub for inference\n",
    "\n",
    "- If you just need inference, run this\n",
    "- we load the model from HFace Hub in 4 bits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb6a35f55c924a43b84b1a8a302c0abb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Attention Sinks] Injected Position Shifting into 32 attention classes.\n",
      "[Attention Sinks] Injected Attention Sink KV Cache into 1 model class.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, TextStreamer, GenerationConfig, BitsAndBytesConfig\n",
    "from attention_sinks import AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=hub_name,\n",
    "                                             trust_remote_code=True,\n",
    "                                             device_map={\"\": 0},\n",
    "                                             attention_sink_size=4,\n",
    "                                             quantization_config=bnb_config,\n",
    "                                            attention_sink_window_size=252, # <- Low for the sake of faster generation\n",
    "                                             )\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Sampling Inference\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST]\n",
      "<<SYS>>\n",
      "provide only one solution and no other possible solution, stick to the main topic, do not introduce any new topics or new question not provided by the student.\n",
      "Make sure the explanations never be longer than 100 words.Don’t justify your answers. <SYS>>\n",
      "\n",
      "Question: I'm working in Cairo 1 :write a  contract that returns the fibonacci of the caller address \n",
      "[/INST]\n",
      "```\n",
      "#[starknet::contract]\n",
      "mod Fib {\n",
      "   #[view]\n",
      "   fn get_fib(self: @ContractState) -> (u256, u256) {\n",
      "       let mut x = 0;\n",
      "       let mut y = 1;\n",
      "       if self.caller == '0x...sn_address_here' {\n",
      "           return (y, x);\n",
      "       } else {\n",
      "           x = x + y;\n",
      "           y = y + x;\n",
      "           return (y, x);\n",
      "        }\n",
      "   }\n",
      "}\n",
      "```\n",
      "\n",
      "This code defines a view function `get_fib`, which checks whether the calling address is equal to a specific address ('0x...sn_address_here'). If it is, it simply returns the values of `y` and `x`. Otherwise, it computes the Fibonacci numbers recursively using `x` and `y`, and then returns them. The `@ContractState` type passed as an argument allows us to access the state variables of the contract.</s>\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer, GenerationConfig\n",
    "\n",
    "\n",
    "#prompt = \"Create an array and append some animal names\"\n",
    "#prompt = \"give an exemple of constructor\"\n",
    "#prompt=\"create an array 'messages' that contains a u128, a u32, a u256\"\n",
    "#prompt = \"create a structure for mailAccount\"\n",
    "#prompt = \"create an array of felt and append 1 to the array\"\n",
    "#prompt = \"create a felt and affect it a value of 1\"\n",
    "\n",
    "#prompt = \"write a  contract that returns the fibonacci of the caller address\"\n",
    "#prompt = \"write an empty contract template\"\n",
    "#prompt = \"what are spans used for\"\n",
    "#prompt = \"How do I know if an array is empty\"\n",
    "#prompt = \"what makes Cairo special\"\n",
    "#prompt = \"Create an array and append some domestic animal names\"\n",
    "text =f\"\"\"[INST]\n",
    "<<SYS>>\n",
    "provide only one solution and no other possible solution, stick to the main topic, do not introduce any new topics or new question not provided by the student.\n",
    "Make sure the explanations never be longer than 100 words.Don’t justify your answers. <SYS>>\n",
    "\n",
    "Question: I'm working in Cairo 1 :{prompt} \n",
    "[/INST]\"\"\"\n",
    "\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    streamer = TextStreamer(tokenizer)\n",
    "    generated_tokens = model.generate(\n",
    "        input_ids,\n",
    "        generation_config=GenerationConfig(\n",
    "            # use_cache=True is required, the rest can be changed up.\n",
    "            use_cache=True,\n",
    "            min_new_tokens=1,\n",
    "            max_new_tokens=1050,\n",
    "            penalty_alpha=0.6,\n",
    "            top_k=280,\n",
    "            do_sample=True,\n",
    "            top_p=0.96,\n",
    "            no_repeat_ngram_size =10,\n",
    "            repetition_penalty = 1.2,\n",
    "            temperature=0.001,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        ),\n",
    "        streamer=streamer,\n",
    "    )\n",
    "    # Decode the final generated text\n",
    "    output_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Beam Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: [INST]\n",
      "<<SYS>>\n",
      "Write explanations and Cairo 1 code  to solve the following coding problem that obeys the constraints and passes the example test cases. Explain the code. Only use Cairo syntax and no other language. Please wrap your code answer using ```\n",
      "</SYS>>Question: I'm working in Cairo 1 :write an empty contract template \n",
      "[/INST]\n",
      "```\n",
      "#[starknet::contract]\n",
      "mod Contract {\n",
      "\n",
      "    #[storage]\n",
      "    struct Storage {\n",
      "        counter: u128,\n",
      "    }\n",
      "\n",
      "    #[external(v0)]\n",
      "    fn set(ref self: ContractState, x: u128) {\n",
      "        self.counter.write(x);\n",
      "    }\n",
      "\n",
      "    #[external(v0)]\n",
      "    fn get(ref self: ContractState) -> u128 {\n",
      "        self.counter.read()\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "```\n",
      "#[starknet::contract]\n",
      "mod Contract {\n",
      "\n",
      "    #[storage]\n",
      "    struct Storage {\n",
      "        counter: u128,\n",
      "    }\n",
      "\n",
      "    #[external(v0)]\n",
      "    fn set(ref self: ContractState, x: u128) {\n",
      "        self.counter.write(x);\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model.eval()\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=200,\n",
    "\n",
    "    num_beams=2,\n",
    "    early_stopping=True,\n",
    "     output_attentions=False,\n",
    "      output_hidden_states=False,\n",
    "      temperature=0.01\n",
    "\n",
    ")\n",
    "#prompt = 'In Cairo 1, write a full sample contract with a constructor that stores the caller into owner'\n",
    "prompt =\"In cairo 1, increment i 6 times using  loop, provide an explanation of the code\"\n",
    "prompt = \"Create an array and append some domestic animal names and write a test to check the value of the item at index 2\"\n",
    "prompt = \" what is an array span ?\"\n",
    "prompt = \"give an exemple of a simplestorage contract with unit tests\"\n",
    "prompt = \"give an exemple of a  contract stores in owner the value of the callers address, add one function that returns the fibonacci value of owner, explain the code\"\n",
    "prompt = \"write an empty contract template\"\n",
    "text =f\"\"\"[INST]\n",
    "<<SYS>>\n",
    "Write explanations and Cairo 1 code  to solve the following coding problem that obeys the constraints and passes the example test cases. Explain the code. Only use Cairo syntax and no other language. Please wrap your code answer using ```\n",
    "</SYS>>Question: I'm working in Cairo 1 :{prompt} \n",
    "[/INST]\"\"\"\n",
    "with torch.no_grad():\n",
    "    sequences = pipeline(\n",
    "    text,\n",
    "\n",
    "        num_return_sequences=1,\n",
    "                bos_token_id=model.config.bos_token_id,\n",
    "                eos_token_id=model.config.eos_token_id,\n",
    "                pad_token_id=model.config.eos_token_id,\n",
    "\n",
    "    )\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cairo-code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
