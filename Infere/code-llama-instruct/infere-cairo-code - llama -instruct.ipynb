{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If in VSCODE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0+cu121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if Torch supports CUDA\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "hub_name = \"StarkWizard/codellama-cairo-instruct\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "running this cell is mandatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e49f3ef4e0e7432683e9bd8f3dc2a4e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bac699921d4497eb2a8d28388fa3a62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00014.bin:   0%|          | 0.00/982M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dd3d3eb4d394469b076429a894ddfa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00014.bin:   0%|          | 0.00/967M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a297ff6c9acd41e4acf6ea041d7cedcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00003-of-00014.bin:   0%|          | 0.00/967M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b78d6abd223d4825bc7eedad4e8ff84c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00004-of-00014.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e817d5e802b486aad2aede8dea838fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00005-of-00014.bin:   0%|          | 0.00/944M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faaf746749b2432dbe8e762644656ecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00006-of-00014.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71fbb4bf40f54d69b6fb4c3dd6843670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00007-of-00014.bin:   0%|          | 0.00/967M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2612235ee9b4a63ae90522651f3450d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00008-of-00014.bin:   0%|          | 0.00/967M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cacafbf9096b4509bb88dc3f84c725c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00009-of-00014.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a71a6f477c4125ad4eddc67c15b09a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00010-of-00014.bin:   0%|          | 0.00/944M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc11fba122d44838994ed4fceac8c898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00011-of-00014.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1080d7f504aa44f8bfe6afdfb92c0b86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00012-of-00014.bin:   0%|          | 0.00/967M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "139eb5c423664565b4b07367e6b2973e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00013-of-00014.bin:   0%|          | 0.00/967M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf9e0c2ec62747dd9239c17f8de8a129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00014-of-00014.bin:   0%|          | 0.00/847M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33737d3f65df46bdba81ee3078834fb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Attention Sinks] Injected Position Shifting into 32 attention classes.\n",
      "[Attention Sinks] Injected Attention Sink KV Cache into 1 model class.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32016, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32016, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, TextStreamer, GenerationConfig, BitsAndBytesConfig\n",
    "from attention_sinks import AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token = \"[PAD]\"\n",
    "tokenizer.padding_side = \"right\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=hub_name,\n",
    "                                             trust_remote_code=True,\n",
    "                                             device_map={\"\": 0},\n",
    "                                             attention_sink_size=4,\n",
    "                                             low_cpu_mem_usage=True,\n",
    "                                             quantization_config=bnb_config,\n",
    "                                            attention_sink_window_size=252, # <- Low for the sake of faster generation\n",
    "                                             )\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model from hub for inference\n",
    "\n",
    "- If you just need inference, run this\n",
    "- we load the model from HFace Hub in 4 bits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Beam Search decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: [INST]\n",
      "<<SYS>>\n",
      "Write explanations and Cairo 1 code  to solve the following coding problem that obeys the constraints and passes the example test cases. Explain the code. Only use Cairo syntax and no other language. Please wrap your code answer using ```\n",
      "</SYS>>Question: I'm working in Cairo 1 :give an exemple of a contract stores the value of the callers address,and provide a function to process the fibonnacci of that address \n",
      "[/INST]\n",
      "```\n",
      "#[starknet::contract]\n",
      "mod Fibonacci {\n",
      "    use starknet::ContractAddress;\n",
      "    use starknet::get_caller_address;\n",
      "\n",
      "    #[storage]\n",
      "    struct Storage {\n",
      "        address: ContractAddress\n",
      "    }\n",
      "\n",
      "    #[external(v0)]\n",
      "    impl Fibonacci of super::starknet::ContractState {\n",
      "\n",
      "        #[constructor]\n",
      "        fn constructor(ref self: ContractState) {\n",
      "            self.address.write(get_caller_address());\n",
      "        }\n",
      "\n",
      "        #[external(v0)]\n",
      "        fn fibonacci(ref self: ContractState) -> u128 {\n",
      "            let address = self.address.read();\n",
      "            let mut result = 0;\n",
      "            let mut previous = 0;\n",
      "            let mut current = 1;\n",
      "            for _i in 0..100 {\n",
      "                result = previous + current;\n",
      "                previous = current;\n",
      "                current = result;\n",
      "            }\n",
      "            result\n",
      "        }\n",
      "    }\n",
      "}\n",
      "```\n",
      " \n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model.eval()\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "\n",
    "    num_beams=2,\n",
    "    early_stopping=True,\n",
    "     output_attentions=False,\n",
    "      output_hidden_states=False,\n",
    "      temperature=0.001\n",
    "\n",
    ")\n",
    "prompt = 'In Cairo 1, write a full sample contract with a constructor that stores the caller into owner'\n",
    "prompt =\"In cairo 1, increment i 6 times using  loop, provide an explanation of the code\"\n",
    "prompt = \"Create an array and append some domestic animal names and write a test to check the value of the item at index 2\"\n",
    "prompt = \" what is an array span ?\"\n",
    "prompt = \"give an exemple of a simplestorage contract with unit tests\"\n",
    "prompt = \"give an exemple of a contract stores the value of the callers address,and provide a function to process the fibonnacci of that address\"\n",
    "#prompt = \"write an empty contract template\"\n",
    "text =f\"\"\"[INST]\n",
    "<<SYS>>\n",
    "Write explanations and Cairo 1 code  to solve the following  problem that obeys the constraints and passes the example test cases. Explain the code. Only use Cairo syntax and no other language. Please wrap your code answer using ```\n",
    "</SYS>>Question: I'm working in Cairo 1 :{prompt} \n",
    "[/INST]\"\"\"\n",
    "with torch.no_grad():\n",
    "    sequences = pipeline(\n",
    "    text,\n",
    "\n",
    "        num_return_sequences=1,\n",
    "                bos_token_id=model.config.bos_token_id,\n",
    "                eos_token_id=model.config.eos_token_id,\n",
    "                pad_token_id=model.config.eos_token_id,\n",
    "\n",
    "    )\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Diverse beam search decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code - llama -instruct.ipynb Cell 15\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X20sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgive an exemple of a  contract stores in owner the value of the callers address, add one function that returns the fibonacci value of owner, explain the code\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X20sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m text \u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\u001b[39m[INST]\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X20sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m<<SYS>>\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X20sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mWrite explanations and Cairo 1 code  to solve the following coding problem that obeys the constraints and passes the example test cases. Explain the code. Only use Cairo syntax and no other language. Please wrap your code answer using ```\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X20sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m</SYS>>Question: I\u001b[39m\u001b[39m'\u001b[39m\u001b[39mm working in Cairo 1 :\u001b[39m\u001b[39m{\u001b[39;00mprompt\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X20sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m[/INST]\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X20sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m sequences \u001b[39m=\u001b[39m pipeline(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X20sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m text,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X20sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X20sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     num_return_sequences\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X20sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m             bos_token_id\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mbos_token_id,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X20sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m             eos_token_id\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X20sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m             pad_token_id\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X20sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     \u001b[39m#max_length=600, # can increase the length of sequence\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X20sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X20sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X20sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m sequences:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X20sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mResult: \u001b[39m\u001b[39m{\u001b[39;00mseq[\u001b[39m'\u001b[39m\u001b[39mgenerated_text\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/pipelines/text_generation.py:208\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, text_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    168\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39m    Complete the prompt(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39m          ids of the generated text.\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(text_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/pipelines/base.py:1140\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\n\u001b[1;32m   1133\u001b[0m         \u001b[39miter\u001b[39m(\n\u001b[1;32m   1134\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1137\u001b[0m         )\n\u001b[1;32m   1138\u001b[0m     )\n\u001b[1;32m   1139\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/pipelines/base.py:1147\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1146\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1147\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1148\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1149\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/pipelines/base.py:1046\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1044\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1045\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m-> 1046\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1047\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m   1048\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/pipelines/text_generation.py:271\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m         generate_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmin_length\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m prefix_length\n\u001b[1;32m    270\u001b[0m \u001b[39m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 271\u001b[0m generated_sequence \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(input_ids\u001b[39m=\u001b[39;49minput_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgenerate_kwargs)\n\u001b[1;32m    272\u001b[0m out_b \u001b[39m=\u001b[39m generated_sequence\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m    273\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/generation/utils.py:1756\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1750\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1751\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[1;32m   1752\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1753\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1754\u001b[0m     )\n\u001b[1;32m   1755\u001b[0m     \u001b[39m# 13. run beam search\u001b[39;00m\n\u001b[0;32m-> 1756\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroup_beam_search(\n\u001b[1;32m   1757\u001b[0m         input_ids,\n\u001b[1;32m   1758\u001b[0m         beam_scorer,\n\u001b[1;32m   1759\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1760\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1761\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1762\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1763\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1764\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1765\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1766\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1767\u001b[0m     )\n\u001b[1;32m   1769\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mCONSTRAINED_BEAM_SEARCH:\n\u001b[1;32m   1770\u001b[0m     final_constraints \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/generation/utils.py:3720\u001b[0m, in \u001b[0;36mGenerationMixin.group_beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3716\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(batch_size):\n\u001b[1;32m   3717\u001b[0m     batch_group_indices\u001b[39m.\u001b[39mextend(\n\u001b[1;32m   3718\u001b[0m         [batch_idx \u001b[39m*\u001b[39m num_beams \u001b[39m+\u001b[39m idx \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(group_start_idx, group_end_idx)]\n\u001b[1;32m   3719\u001b[0m     )\n\u001b[0;32m-> 3720\u001b[0m group_input_ids \u001b[39m=\u001b[39m input_ids[batch_group_indices]\n\u001b[1;32m   3722\u001b[0m \u001b[39m# select outputs of beams of current group only\u001b[39;00m\n\u001b[1;32m   3723\u001b[0m next_token_logits \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlogits[batch_group_indices, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "\n",
    "tokenizer.pad_token = tokenizer.bos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "\n",
    "\n",
    "    num_beams=5,\n",
    "    num_beam_groups=5,\n",
    "    diversity_penalty=2.0,\n",
    "    early_stopping=True,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    "    max_new_tokens=512,\n",
    "      num_return_sequences=1,\n",
    "\n",
    ")\n",
    "#prompt = 'In Cairo 1, write a full sample contract with a constructor that stores the caller into owner'\n",
    "prompt =\"In cairo 1, increment i 6 times using  loop, provide an explanation of the code\"\n",
    "prompt = \"Create an array and append some domestic animal names and write a test to check the value of the item at index 2\"\n",
    "prompt = \" what is an array span ?\"\n",
    "prompt = \"give an exemple of a simplestorage contract with unit tests\"\n",
    "prompt = \"give an exemple of a  contract stores in owner the value of the callers address, add one function that returns the fibonacci value of owner, explain the code\"\n",
    "\n",
    "text =f\"\"\"[INST]\n",
    "<<SYS>>\n",
    "Write explanations and Cairo 1 code  to solve the following coding problem that obeys the constraints and passes the example test cases. Explain the code. Only use Cairo syntax and no other language. Please wrap your code answer using ```\n",
    "</SYS>>Question: I'm working in Cairo 1 :{prompt} \n",
    "[/INST]\"\"\"\n",
    "\n",
    "sequences = pipeline(\n",
    "text,\n",
    "\n",
    "    num_return_sequences=1,\n",
    "            bos_token_id=model.config.bos_token_id,\n",
    "            eos_token_id=model.config.eos_token_id,\n",
    "            pad_token_id=model.config.eos_token_id,\n",
    "    #max_length=600, # can increase the length of sequence\n",
    "\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Beam Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=512) and `max_length`(=600) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code - llama -instruct.ipynb Cell 17\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X22sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m text \u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\u001b[39m[INST]\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X22sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m<<SYS>>\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X22sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mWrite explanations and Cairo 1 code  to solve the following coding problem that obeys the constraints and passes the example test cases. Explain the code. Only use Cairo syntax and no other language. Please wrap your code answer using ```\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X22sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m</SYS>>Question: I\u001b[39m\u001b[39m'\u001b[39m\u001b[39mm working in Cairo 1 :\u001b[39m\u001b[39m{\u001b[39;00mprompt\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X22sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m[/INST]\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X22sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X22sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     sequences \u001b[39m=\u001b[39m pipeline(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X22sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     text,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X22sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X22sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m         num_return_sequences\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X22sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m                 bos_token_id\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mbos_token_id,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X22sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m                 eos_token_id\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X22sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m                 pad_token_id\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X22sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m         max_length\u001b[39m=\u001b[39;49m\u001b[39m600\u001b[39;49m, \u001b[39m# can increase the length of sequence\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X22sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X22sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m sequences:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X22sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mResult: \u001b[39m\u001b[39m{\u001b[39;00mseq[\u001b[39m'\u001b[39m\u001b[39mgenerated_text\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/pipelines/text_generation.py:208\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, text_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    168\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39m    Complete the prompt(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39m          ids of the generated text.\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(text_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/pipelines/base.py:1140\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\n\u001b[1;32m   1133\u001b[0m         \u001b[39miter\u001b[39m(\n\u001b[1;32m   1134\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1137\u001b[0m         )\n\u001b[1;32m   1138\u001b[0m     )\n\u001b[1;32m   1139\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/pipelines/base.py:1147\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1146\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1147\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1148\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1149\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/pipelines/base.py:1046\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1044\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1045\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m-> 1046\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1047\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m   1048\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/pipelines/text_generation.py:271\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m         generate_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmin_length\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m prefix_length\n\u001b[1;32m    270\u001b[0m \u001b[39m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 271\u001b[0m generated_sequence \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(input_ids\u001b[39m=\u001b[39;49minput_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgenerate_kwargs)\n\u001b[1;32m    272\u001b[0m out_b \u001b[39m=\u001b[39m generated_sequence\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m    273\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/generation/utils.py:1722\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1714\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1715\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1716\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[1;32m   1717\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1718\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1719\u001b[0m     )\n\u001b[1;32m   1721\u001b[0m     \u001b[39m# 14. run beam sample\u001b[39;00m\n\u001b[0;32m-> 1722\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbeam_sample(\n\u001b[1;32m   1723\u001b[0m         input_ids,\n\u001b[1;32m   1724\u001b[0m         beam_scorer,\n\u001b[1;32m   1725\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1726\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mlogits_warper,\n\u001b[1;32m   1727\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1728\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1729\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1730\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1731\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1732\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1733\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1734\u001b[0m     )\n\u001b[1;32m   1736\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[1;32m   1737\u001b[0m     \u001b[39m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m     beam_scorer \u001b[39m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1739\u001b[0m         batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1740\u001b[0m         num_beams\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1746\u001b[0m         max_length\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mmax_length,\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/generation/utils.py:3350\u001b[0m, in \u001b[0;36mGenerationMixin.beam_sample\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3346\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m   3348\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[0;32m-> 3350\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   3351\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   3352\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   3353\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   3354\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   3355\u001b[0m )\n\u001b[1;32m   3357\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   3358\u001b[0m     cur_len \u001b[39m=\u001b[39m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:1038\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1035\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   1037\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1038\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m   1039\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1040\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1041\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1042\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1043\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1044\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1045\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1046\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1047\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1048\u001b[0m )\n\u001b[1;32m   1050\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1051\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpretraining_tp \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/attention_sinks/inject_mixin.py:131\u001b[0m, in \u001b[0;36mInjectAttentionSinksMixin._inject_attention_sink_kv_cache.<locals>.overwrite_forward.<locals>.wrapped_forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_forward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 131\u001b[0m     outputs \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    132\u001b[0m     outputs\u001b[39m.\u001b[39mpast_key_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention_sink_kv_cache(outputs\u001b[39m.\u001b[39mpast_key_values)\n\u001b[1;32m    133\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:925\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    921\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    922\u001b[0m         create_custom_forward(decoder_layer), hidden_states, attention_mask, position_ids\n\u001b[1;32m    923\u001b[0m     )\n\u001b[1;32m    924\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 925\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    926\u001b[0m         hidden_states,\n\u001b[1;32m    927\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    928\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    929\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    930\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    931\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    932\u001b[0m         padding_mask\u001b[39m=\u001b[39;49mpadding_mask,\n\u001b[1;32m    933\u001b[0m     )\n\u001b[1;32m    935\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    937\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:632\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m    618\u001b[0m \u001b[39m    hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39m    past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    630\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[0;32m--> 632\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_layernorm(hidden_states)\n\u001b[1;32m    634\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[1;32m    635\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attn(\n\u001b[1;32m    636\u001b[0m     hidden_states\u001b[39m=\u001b[39mhidden_states,\n\u001b[1;32m    637\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    642\u001b[0m     padding_mask\u001b[39m=\u001b[39mpadding_mask,\n\u001b[1;32m    643\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:112\u001b[0m, in \u001b[0;36mLlamaRMSNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    110\u001b[0m hidden_states \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m    111\u001b[0m variance \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39mpow(\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mmean(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 112\u001b[0m hidden_states \u001b[39m=\u001b[39m hidden_states \u001b[39m*\u001b[39;49m torch\u001b[39m.\u001b[39;49mrsqrt(variance \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvariance_epsilon)\n\u001b[1;32m    113\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m*\u001b[39m hidden_states\u001b[39m.\u001b[39mto(input_dtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model.eval()\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "\n",
    "    num_beams=3,\n",
    "    do_sample=True,\n",
    "    early_stopping=True,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    "    temperature=0.9,\n",
    "    top_k=200,\n",
    "    top_p=1.5,\n",
    "\n",
    ")\n",
    "#prompt = 'In Cairo 1, write a full sample contract with a constructor that stores the caller into owner'\n",
    "prompt =\"In cairo 1, increment i 6 times using  loop, provide an explanation of the code\"\n",
    "prompt = \"Create an array and append some domestic animal names and write a test to check the value of the item at index 2\"\n",
    "prompt = \" what is an array span ?\"\n",
    "prompt = \"give an exemple of a simplestorage contract with unit tests\"\n",
    "prompt = \"give an exemple of a  contract stores in owner the value of the callers address, add one function that returns the fibonacci value of owner, explain the code\"\n",
    "\n",
    "text =f\"\"\"[INST]\n",
    "<<SYS>>\n",
    "Write explanations and Cairo 1 code  to solve the following coding problem that obeys the constraints and passes the example test cases. Explain the code. Only use Cairo syntax and no other language. Please wrap your code answer using ```\n",
    "</SYS>>Question: I'm working in Cairo 1 :{prompt} \n",
    "[/INST]\"\"\"\n",
    "with torch.no_grad():\n",
    "    sequences = pipeline(\n",
    "    text,\n",
    "\n",
    "        num_return_sequences=1,\n",
    "                bos_token_id=model.config.bos_token_id,\n",
    "                eos_token_id=model.config.eos_token_id,\n",
    "                pad_token_id=model.config.eos_token_id,\n",
    "        max_length=600, # can increase the length of sequence\n",
    "    )\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Contrastive method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST]\n",
      "<<SYS>>\n",
      "Write Cairo 1 code to solve the following coding problem that obeys the constraints and passes the example test cases. Only use Cairo syntax and no other language. Please wrap your code answer using ```\n",
      "<</SYS>>\n",
      "Question: I'm working in Cairo 1 :give an exemple of a  contract stores in owner the value of the callers address, add one function that returns the fibonacci value of owner, explain the code \n",
      "[/INST]\n",
      "```\n",
      "#[starknet::contract]\n",
      "mod fibonacci{\n",
      "   use starknet::ContractAddress;\n",
      "   use starknet::get_caller_address;\n",
      "   use starknet::syscalls::library_call_syscall;\n",
      "   use starknet::syscalls::starknet_keccak;\n",
      "   use "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code - llama -instruct.ipynb Cell 19\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X24sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X24sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     streamer \u001b[39m=\u001b[39m TextStreamer(tokenizer)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X24sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     generated_tokens \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X24sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m         input_ids,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X24sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m         generation_config\u001b[39m=\u001b[39;49mGenerationConfig(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X24sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m             \u001b[39m# use_cache=True is required, the rest can be changed up.\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X24sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m             use_cache\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X24sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m             min_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X24sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m             max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m1050\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X24sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X24sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     penalty_alpha\u001b[39m=\u001b[39;49m\u001b[39m0.35\u001b[39;49m, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X24sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     top_k\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X24sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X24sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m             bos_token_id\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mbos_token_id,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X24sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m             eos_token_id\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X24sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m             pad_token_id\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X24sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m         ),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X24sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X24sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X24sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     \u001b[39m# Decode the final generated text\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X24sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     output_text \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(generated_tokens[\u001b[39m0\u001b[39m], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/generation/utils.py:1623\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1620\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m   1621\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mContrastive search requires `use_cache=True`\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1623\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontrastive_search(\n\u001b[1;32m   1624\u001b[0m         input_ids,\n\u001b[1;32m   1625\u001b[0m         top_k\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mtop_k,\n\u001b[1;32m   1626\u001b[0m         penalty_alpha\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpenalty_alpha,\n\u001b[1;32m   1627\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1628\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1629\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1630\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1631\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1632\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1633\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1634\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1635\u001b[0m         sequential\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mlow_memory,\n\u001b[1;32m   1636\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1637\u001b[0m     )\n\u001b[1;32m   1639\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mSAMPLE:\n\u001b[1;32m   1640\u001b[0m     \u001b[39m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1641\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/generation/utils.py:2154\u001b[0m, in \u001b[0;36mGenerationMixin.contrastive_search\u001b[0;34m(self, input_ids, top_k, penalty_alpha, logits_processor, logits_warper, stopping_criteria, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, sequential, **model_kwargs)\u001b[0m\n\u001b[1;32m   2150\u001b[0m \u001b[39m# compute the degeneration penalty and re-rank the candidates based on the degeneration penalty and the\u001b[39;00m\n\u001b[1;32m   2151\u001b[0m \u001b[39m# model confidence. Keeping `selected_idx` on CPU enables multi-device contrastive search and doesn't\u001b[39;00m\n\u001b[1;32m   2152\u001b[0m \u001b[39m# introduce (noticeable) slowdowns on single-device runs.\u001b[39;00m\n\u001b[1;32m   2153\u001b[0m selected_idx \u001b[39m=\u001b[39m _ranking_fast(context_hidden, next_hidden, top_k_probs, penalty_alpha, top_k)\n\u001b[0;32m-> 2154\u001b[0m selected_idx \u001b[39m=\u001b[39m selected_idx\u001b[39m.\u001b[39;49mto(\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   2156\u001b[0m \u001b[39m# prepare for the next step: (1) next token_id; (2) past_key_values; (3) last_hidden_states for computing\u001b[39;00m\n\u001b[1;32m   2157\u001b[0m \u001b[39m# the degeneration penalty; (4) logits for selecting next top-k candidates; (5) selected tokens scores\u001b[39;00m\n\u001b[1;32m   2158\u001b[0m \u001b[39m# (model confidence minus degeneration penalty); (6) decoder hidden_states\u001b[39;00m\n\u001b[1;32m   2159\u001b[0m next_tokens \u001b[39m=\u001b[39m top_k_ids[\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(top_k_ids)), selected_idx]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer, GenerationConfig, StoppingCriteriaList,MaxLengthCriteria\n",
    "\n",
    "tokenizer.padding_side='right'\n",
    "\n",
    "#prompt = \"Create an array and append some animal names\"\n",
    "#prompt = \"give an exemple of constructor\"\n",
    "#prompt=\"create an array 'exempl' that contains a u128, a u32, a u256\"\n",
    "prompt=\"provide a a contract with an external function that returns the fibonacci of the caller's address\"\n",
    "#prompt = \"create a structure for mailAccount\"\n",
    "#prompt = \"create an array of felt and append 1 to the array\"\n",
    "#prompt = \"create a felt and affect it a value of 1\"\n",
    "#prompt=\"create a function for fibonacci\"\n",
    "#prompt = \"what are spans used for\"\n",
    "#prompt = \"How do I know if an array is empty\"\n",
    "prompt =\"increment i 6 times using loop\"\n",
    "#prompt = \"what makes Cairo special\"\n",
    "#prompt = \"Create an array and append some domestic animal names\"\n",
    "#prompt = \"Create an array and append domestic animal names\"\n",
    "prompt = \"write a contract with a add function that returns the sum of 2 parameters\"\n",
    "#prompt = 'In Cairo 1, write a full sample contract with a constructor that stores the caller into owner''\n",
    "prompt = \"give an exemple of a  contract stores in owner the value of the callers address, add one function that returns the fibonacci value of owner, explain the code\"\n",
    "\n",
    "text =f\"\"\"[INST]\n",
    "<<SYS>>\n",
    "Write Cairo 1 code to solve the following coding problem that obeys the constraints and passes the example test cases. Only use Cairo syntax and no other language. Please wrap your code answer using ```\n",
    "<</SYS>>\n",
    "Question: I'm working in Cairo 1 :{prompt} \n",
    "[/INST]\"\"\"\n",
    "\n",
    "token_input = tokenizer(text, return_tensors='pt')\n",
    "input_ids = token_input['input_ids'].cuda()\n",
    "attention_mask = token_input['attention_mask'].cuda()\n",
    "pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=1)])\n",
    "\n",
    "with torch.no_grad():\n",
    "    streamer = TextStreamer(tokenizer)\n",
    "    generated_tokens = model.generate(\n",
    "        input_ids,\n",
    "        generation_config=GenerationConfig(\n",
    "            # use_cache=True is required, the rest can be changed up.\n",
    "            use_cache=True,\n",
    "            min_new_tokens=1,\n",
    "            max_new_tokens=1050,\n",
    "\n",
    "    penalty_alpha=0.35, \n",
    "    top_k=4,\n",
    "    attention_mask=attention_mask,\n",
    "            bos_token_id=model.config.bos_token_id,\n",
    "            eos_token_id=model.config.eos_token_id,\n",
    "            pad_token_id=model.config.eos_token_id,\n",
    "        ),\n",
    "        streamer=streamer,\n",
    "    )\n",
    "    # Decode the final generated text\n",
    "    output_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "How do I know if an array is empty\n",
      "\n",
      "Answer:\n",
      "\n",
      "    // You can use the is_empty() method to check if an array is empty.\n",
      "    // If the array is empty, is_empty() returns true.\n",
      "    // If the array is not empty, is_empty() returns false.\n",
      "    // You can also use the len() method to check the length of an array.\n",
      "    // If the array is empty, len() returns 0.\n",
      "    // If the array is not empty, len() returns a positive integer.\n",
      "    // You can also use the is_empty() method to check if an array is empty.\n",
      "    // If the array is empty, is_empty() returns true.\n",
      "    // If the array is not empty, is_empty() returns false.\n",
      "    // You can also use the len() method to check the length of an array.\n",
      "    // If the array is empty, len() returns 0.\n",
      "    // If the array is not\n"
     ]
    }
   ],
   "source": [
    "#prompt = \"Create an array and append some animal names\"\n",
    "#prompt=\"create an array 'messages' that contains a u128, a u32, a u256\"\n",
    "#prompt = \"create a structure for mailAccount\"\n",
    "#prompt = \"create an array of felt and append 1 to the array\"\n",
    "#prompt = \"create a felt and affect it a value of 1\"\n",
    "#prompt=\"create a function for fibonacci\"\n",
    "#prompt = \"what are spans used for\"\n",
    "prompt = \"How do I know if an array is empty\"\n",
    "#prompt = \"what are spans used for\"\n",
    "text =f\"[INST]I'm working in Cairo. You are a cairo expert and know no other computer language, answer in less than 170 words{prompt} [/INST]\"\n",
    "\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "\n",
    "outputs = model.generate(input_ids=input_ids,       \n",
    "    max_new_tokens=200,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.01,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "\n",
    "print(f\"Prompt:\\n{prompt}\\n\")\n",
    "print(f\"Answer:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(text):]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "```\n",
      "   #[derive(Copy, Drop)] // Implement Copy trait to allow copying of the struct instance when passing it as argument or returning it from function\n",
      "                        // Implement Drop trait to automatically drop (destroy) instances of this type when they go out of scope\n",
      "   struct User {\n",
      "       active: bool,\n",
      "       username: felt252,\n",
      "       email: felt252,\n",
      "       sign_in_count: u64,\n",
      "   }\n",
      "```\n",
      "\n",
      "The above code defines a struct called 'User'. It has five fields: 'active', which is a boolean value indicating whether the user account is currently active; 'username', which stores the username for the current user; 'email', which stores an email address for the user; 'sign_in_count', which counts how many times the user signed in; and lastly, 'created_at', which stores the timestamp at which the user was created. Each field is defined using its appropriate data type. The first four fields use primitive types commonly used throughout programming. The fifth field uses a custom type, 'felt252', which represents a string type. This choice allows us to store strings efficiently within our program without relying on external libraries.  \n",
      "Note that each time we define a new struct, we must include the 'use std::{felt252};' statement at the top of our source file to bring into scope the required definitions for the custom type. We will see more examples of defining custom types later in this chapter.\n",
      "\n",
      "In addition to these built-in numeric and Boolean types, Rust also supports arrays, tuples, dictionaries, enums, and custom types. These different types serve distinct purposes but can be combined to create complex structures. Understanding their roles and how they interact helps you write robust programs.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer, GenerationConfig\n",
    "\n",
    "\n",
    "prompt = \"Create an array and append domestic animal names\"\n",
    "prompt = \"give an exemple of a simplestorage contract with unit tests\"\n",
    "#prompt = \"create a felt and affect it a value of 1\"\n",
    "#prompt=\"In the example 'let y = { let x = 3; x + 1 };', what is the value of y? give a similar sample\"\n",
    "#prompt=\"How can you print a variable's value in Cairo? Give a full sample using an array\"\n",
    "#prompt = \"hat type of operations can be considered expressions in Cairo? Give an exemple.\"\n",
    "#prompt = \"What is a characteristic feature of Felt252Dict<T> when interacting with it? Give a sample\"\n",
    "#prompt = \"How do I create a span ? \"\n",
    "#prompt = \"How do I know if an array is empty, give 2 examples\"\n",
    "#prompt = \"what makes Cairo special\"\n",
    "#prompt = \"Create an array and append some domestic animal names\"\n",
    "#prompt=\"write a contract that computes the fibonacci of caller's address and explain the weakness of the program\"\n",
    "#prompt=\"write a contract that returns the fibonacci of a value passed as parameter. implement the test function for the value 10\"\n",
    "#prompt=\"write an empty contract template and give the instructions to compile and deploy it\"\n",
    "#prompt=\"write a contract that stores the caller's address in its constructor and a function that returns the fibonnacci of that address\"\n",
    "#prompt=\"How can you implement the `Copy` trait on a custom type in Cairo?\"\n",
    "#prompt = \"What's the significance of the `self` parameter in my method of the `CubeGeom` trait?\"\n",
    "#prompt = \"write a contract with a function that returns the address of the caller\"\n",
    "#prompt=\"how can i cast a variable into another ?\"\n",
    "\n",
    "#prompt = \"What do the terms 'parent' and 'child' mean in the context of modules?\"\n",
    "#prompt=\"What can be defined in trait and impl blocks related to a type?\"\n",
    "#prompt = \"How do I know if an array is empty, give 2 examples\"\n",
    "\n",
    "#rompt=\"write a  basic simple contract that returns the fibonacci of the caller's address\"\n",
    "prompt = \"Create an array and append some domestic animal names as string\"\n",
    "\n",
    "\n",
    "\n",
    "#prompt = \"How do I know if an array is empty, give 2 examples \"\n",
    "\n",
    "prompt = \"Provide a basic code snippet that defines a struct named 'User' with fields like 'active', 'username', and so on\"\n",
    "\n",
    "\n",
    "#prompt=\"write an empty contract template and give the instructions to compile and deploy it\"\n",
    "\n",
    "#prompt = \"write a contract with a function that returns the address of the caller\"\n",
    "#prompt = \"write a contract with a 'add_felt' function that gets the sum of 2 felt parameters\"\n",
    "#prompt = \"Create an array and append 3 domestic animal names as string and print the the second element\"\n",
    "#prompt = \"write a contract that returns the fibonacci of the caller address\"\n",
    "\n",
    "#prompt=\"write a  contract that provides a function that takes two arrays as parameter and store an array composed of the sum of the element of the arrays\"\n",
    "#prompt = \"What's the difference between Nullable<T> and Option in terms of where the wrapped value is stored?\"\n",
    "\n",
    "\n",
    "#prompt = \"How does the Cairo convention style name functions and variables?\"\n",
    "\n",
    "#prompt = \"write a  contract that returns the fibonacci(caller address)\"\n",
    "\n",
    "#prompt = \"What does the #[starknet::component] attribute signify?\"\n",
    "\n",
    "#prompt = \" What is the naming convention for enum variants, give some examples\"\n",
    "\n",
    "#prompt =\"\"\"#[test]\n",
    "#[available_gas(200000)]\n",
    "#fn test_loop() {\n",
    "#    let mut counter = 0;\n",
    "#    //TODO make the test pass without changing any existing line\n",
    "#    loop {\n",
    "#        counter += 1;\n",
    "#    };\n",
    "#    assert(counter == 140, 'counter should be 140')\n",
    "#}\n",
    "#\"\"\"\n",
    "\n",
    "#prompt = \"write a contract for implementing a function that computes u(n)=u(n-1)+1, with n passed as parameter\"\n",
    "\n",
    "text =f\"\"\"[INST]\n",
    "<<SYS>>\n",
    "A student asks you a question about Cairo 1. Provide a concise answer to the student's questions,do not expand the subject of the question, do not introduce any new topics or new question not provided by the student.\n",
    "Make sure the explanations never be longer than 300 words.Don’t justify your answers. Don’t give information not mentioned in the CONTEXT INFORMATION.provide only one solution <SYS>>\n",
    "\n",
    "Question: I'm working in Cairo 1 :{prompt} \n",
    "[/INST]\"\"\"\n",
    "\n",
    "text =f\"[INST]I'm working in Cairo. You are a cairo expert and know no other computer language, answer in less than 170 words{prompt} [/INST]\"\n",
    "\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "    generated_tokens = model.generate(\n",
    "        input_ids,\n",
    "        generation_config=GenerationConfig(\n",
    "                       # use_cache=True is required, the rest can be changed up.\n",
    "            use_cache=True,\n",
    "            min_new_tokens=1,\n",
    "            max_new_tokens=1050,\n",
    "            penalty_alpha=0.1\n",
    "            ,\n",
    "            top_k=2000,\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "\n",
    "            repetition_penalty =1.2,\n",
    "            temperature=0.001,\n",
    "            early_stopping=True,\n",
    "            bos_token_id=model.config.bos_token_id,\n",
    "            eos_token_id=model.config.eos_token_id,\n",
    "            pad_token_id=model.config.eos_token_id,\n",
    "        ),\n",
    "        streamer=streamer,\n",
    "    )\n",
    "    # Decode the final generated text\n",
    "    output_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
