{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If in VSCODE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if Torch supports CUDA\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "hub_name = \"StarkWizard/codellama-cairo-instruct\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "running this cell is mandatory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model from hub for inference\n",
    "\n",
    "- If you just need inference, run this\n",
    "- we load the model from HFace Hub in 4 bits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e12314ce2a643b2b7c146b6c5fc963d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Attention Sinks] Injected Position Shifting into 32 attention classes.\n",
      "[Attention Sinks] Injected Attention Sink KV Cache into 1 model class.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32016, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32016, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, TextStreamer, GenerationConfig, BitsAndBytesConfig\n",
    "from attention_sinks import AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=hub_name,\n",
    "                                             trust_remote_code=True,\n",
    "                                             device_map={\"\": 0},\n",
    "                                             attention_sink_size=4,\n",
    "                                             low_cpu_mem_usage=True,\n",
    "                                             quantization_config=bnb_config,\n",
    "                                            attention_sink_window_size=252, # <- Low for the sake of faster generation\n",
    "                                             )\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST]I'm working in Cairo language and need a concise response. You are a cairo expert, answer the question exactly and be concise, no more than 3 explanations or remarks.: provide a contract to handle whitelist, give instructions to compile and deploy the contract [/INST]\n",
      "#[derive(Copy, Drop)]\n",
      "enum Role {\n",
      "   Admin: (),\n",
      "   Member: (),\n",
      "}\n",
      "\n",
      "struct Whitelist<T> {\n",
      "   roles: T,\n",
      "}\n",
      "\n",
      "fn main() {\n",
      "   let mut whitelist = Whitelist::<Role>::new();\n",
      "\n",
      "   // Add an admin\n",
      "   whitelist.add_role(Role::Admin(()));\n",
      "\n",
      "   // Add a member\n",
      "   whitelist.add_role(Role::Member(()));\n",
      "}\n",
      "\n",
      "impl<T> Whitelist<T> {\n",
      "   fn new() -> Self {\n",
      "       Self {\n",
      "           roles: T::default(),\n",
      "       }\n",
      "   }\n",
      "\n",
      "   fn add_role(&mut self, role: T) {\n",
      "       self.roles = T::combine(self.roles, role);\n",
      "   }\n",
      "}\n",
      "\n",
      "Explanations:\n",
      "- We define an enum Role to represent the two roles we want to manage in the whitelist contract.\n",
      "- We then define a struct Whitelist that has a single field roles of type T, which we genericize over the Role enum.\n",
      "- We implement the new method to return an instance of Whitelist<T>\n",
      "- We then implement the add_role method to update the value of self.roles with the new role passed as parameter.\n",
      "- We use the combine method from the trait Combine to combine the new role with the existing roles.\n",
      "\n",
      "Note: In this implementation, we assume that the roles are binary and can be combined using the combine method from the trait Combine.\n",
      "\n",
      "This implementation allows us to add new roles to the whitelist contract without modifying the existing code.\n",
      "\n",
      "This is a very basic implementation of a whitelist, in a real-world scenario you would want to add more functionality like removing users from the whitelist, managing permissions, etc.\n",
      "\n",
      "Also, you can use other traits like Combine, which allows you to combine two values of the same type.\n",
      "\n",
      "It's important to note that this implementation is very basic and can be improved in many ways, but it serves as a basic example of how to implement a whitelist using Rust.\n",
      "\n",
      "Also, you can use other traits like Combine, which allows you to combine two values of the same type.\n",
      "\n",
      "It's important to note that this implementation is very basic and can be improved in many ways, but it serves as a basic example of how to implement a whitelist using Rust.\n",
      "\n",
      "Also, you can use other traits like Combine, which allows you to combine two values of the same type.\n",
      "\n",
      "It's important to note that this implementation is very basic and can be improved in many ways, but it serves as a basic example of how to implement a whitelist using Rust.\n",
      "\n",
      "Also, you can use other traits like Combine, which allows you to combine two values of the same type.\n",
      "\n",
      "It's important to note that this implementation is very basic and can be improved in many ways, but it serves as a basic example of how to implement a whitelist using Rust.\n",
      "\n",
      "Also, you can use other traits like Combine, which allows you to combine two values of the same type.\n",
      "\n",
      "It's important to note that this implementation is very basic and can be improved in many ways, but it serves as a basic example of how to implement a whitelist using Rust.\n",
      "\n",
      "Also, you can use other traits like Combine, which allows you to combine two values of the same type.\n",
      "\n",
      "It's important to note that this implementation is very basic and can be improved in many ways, but it serves as a basic example of how to implement a whitelist using Rust.\n",
      "\n",
      "Also, you can use other traits like Combine, which allows you to combine two values of the same type.\n",
      "\n",
      "It's important to note that this implementation is very basic and can be improved in many ways, but it serves as a basic example of how to implement a whitelist using Rust.\n",
      "\n",
      "Also, you can use other traits like Combine, which allows you to combine two values of the same type.\n",
      "\n",
      "It's important to note that this implementation is very basic and can be improved in many ways, but it serves as a basic example of how to implement a whitelist using Rust.\n",
      "\n",
      "Also, you can use other traits like Combine, which allows you to combine two values of the same type.\n",
      "\n",
      "It's important to note that this implementation is very basic and can be improved in many ways, but\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer, GenerationConfig, StoppingCriteriaList,MaxLengthCriteria\n",
    "\n",
    "\n",
    "#prompt = \"Create an array and append some animal names\"\n",
    "#prompt = \"give an exemple of constructor\"\n",
    "#prompt=\"create an array 'exempl' that contains a u128, a u32, a u256\"\n",
    "prompt=\"provide a contract to handle whitelist, give instructions to compile and deploy the contract\"\n",
    "#prompt = \"create a structure for mailAccount\"\n",
    "#prompt = \"create an array of felt and append 1 to the array\"\n",
    "#prompt = \"create a felt and affect it a value of 1\"\n",
    "#prompt=\"create a function for fibonacci\"\n",
    "#prompt = \"what are spans used for\"\n",
    "#prompt = \"How do I know if an array is empty\"\n",
    "#prompt = \"what makes Cairo special\"\n",
    "#prompt = \"Create an array and append some domestic animal names\"\n",
    "\n",
    "text =f\"[INST]I'm working in Cairo language and need a concise response. You are a cairo expert, answer the question exactly and be concise, no more than 3 explanations or remarks.: {prompt} [/INST]\"\n",
    "\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(model.device)\n",
    "stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=1)])\n",
    "\n",
    "with torch.no_grad():\n",
    "    streamer = TextStreamer(tokenizer)\n",
    "    generated_tokens = model.generate(\n",
    "        input_ids,\n",
    "        generation_config=GenerationConfig(\n",
    "            # use_cache=True is required, the rest can be changed up.\n",
    "            use_cache=True,\n",
    "            min_new_tokens=1,\n",
    "            max_new_tokens=1000,\n",
    "            penalty_alpha=1.0,\n",
    "            top_k=40,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.03,\n",
    "            stopping_criteria=stopping_criteria,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            early_stopping=True,\n",
    "        ),\n",
    "        streamer=streamer,\n",
    "    )\n",
    "    # Decode the final generated text\n",
    "    output_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "what are spans used for\n",
      "\n",
      "Answer:\n",
      "\n",
      "\n",
      "Span is a useful type when you want to extend an existing type without affecting the code that uses it. It allows you to add new fields to a value without affecting the existing code that uses the value. This makes it a versatile tool for building flexible and maintainable code.\n",
      "\n",
      "For example, imagine you have a structure called User, which has fields like name, email, and password. You might want to add a new field called 'age' to the User structure in the future. Using a span allows you to do so without affecting the existing code that uses the User structure.\n",
      "\n",
      "In summary, spans are useful when you want to extend an existing type without affecting the code that uses it. They allow you to add new fields to a value without affecting the existing code that uses the value, making them a versatile tool for building flexible and maintainable code.\n",
      "\n",
      "Note: Spans are immutable, which means that once you\n"
     ]
    }
   ],
   "source": [
    "#prompt = \"Create an array and append some animal names\"\n",
    "#prompt=\"create an array 'messages' that contains a u128, a u32, a u256\"\n",
    "#prompt = \"create a structure for mailAccount\"\n",
    "#prompt = \"create an array of felt and append 1 to the array\"\n",
    "#prompt = \"create a felt and affect it a value of 1\"\n",
    "#prompt=\"create a function for fibonacci\"\n",
    "#prompt = \"what are spans used for\"\n",
    "#prompt = \"How do I know if an array is empty\"\n",
    "prompt = \"what are spans used for\"\n",
    "text =f\"[INST]I'm working in Cairo. You are a cairo expert and know no other computer language, answer in less than 170 words{prompt} [/INST]\"\n",
    "\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "\n",
    "outputs = model.generate(input_ids=input_ids,       \n",
    "    max_new_tokens=200,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.01,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "\n",
    "print(f\"Prompt:\\n{prompt}\\n\")\n",
    "print(f\"Answer:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(text):]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
