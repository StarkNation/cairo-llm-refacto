
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/pechaut/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/datasets/load.py:2097: FutureWarning: 'ignore_verifications' was deprecated in favor of 'verification_mode' in version 2.9.1 and will be removed in 3.0.0.
You can remove this warning by passing 'verification_mode=no_checks' instead.
  warnings.warn(
/home/pechaut/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:226: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/pechaut/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 5.0247, 'learning_rate': 5.319148936170213e-07, 'epoch': 0.0}
{'eval_loss': 4.652410984039307, 'eval_runtime': 0.9993, 'eval_samples_per_second': 5.004, 'eval_steps_per_second': 1.001, 'epoch': 0.0}
{'loss': 3.8615, 'learning_rate': 1.0638297872340427e-06, 'epoch': 0.0}
{'eval_loss': 4.649815082550049, 'eval_runtime': 1.0055, 'eval_samples_per_second': 4.972, 'eval_steps_per_second': 0.994, 'epoch': 0.0}
{'loss': 4.8416, 'learning_rate': 1.595744680851064e-06, 'epoch': 0.0}
{'eval_loss': 4.645392894744873, 'eval_runtime': 1.0211, 'eval_samples_per_second': 4.897, 'eval_steps_per_second': 0.979, 'epoch': 0.0}
{'loss': 4.5876, 'learning_rate': 2.1276595744680853e-06, 'epoch': 0.01}
{'eval_loss': 4.639204978942871, 'eval_runtime': 1.0327, 'eval_samples_per_second': 4.842, 'eval_steps_per_second': 0.968, 'epoch': 0.01}
{'loss': 4.9639, 'learning_rate': 2.6595744680851065e-06, 'epoch': 0.01}
{'eval_loss': 4.6306538581848145, 'eval_runtime': 1.0538, 'eval_samples_per_second': 4.745, 'eval_steps_per_second': 0.949, 'epoch': 0.01}
{'loss': 5.3684, 'learning_rate': 3.191489361702128e-06, 'epoch': 0.01}
{'eval_loss': 4.6198344230651855, 'eval_runtime': 1.1708, 'eval_samples_per_second': 4.27, 'eval_steps_per_second': 0.854, 'epoch': 0.01}
{'loss': 3.4207, 'learning_rate': 3.723404255319149e-06, 'epoch': 0.01}
{'eval_loss': 4.601974010467529, 'eval_runtime': 1.0942, 'eval_samples_per_second': 4.57, 'eval_steps_per_second': 0.914, 'epoch': 0.01}
{'loss': 4.3562, 'learning_rate': 4.255319148936171e-06, 'epoch': 0.01}
{'eval_loss': 4.5785956382751465, 'eval_runtime': 1.1325, 'eval_samples_per_second': 4.415, 'eval_steps_per_second': 0.883, 'epoch': 0.01}
{'loss': 4.3054, 'learning_rate': 4.787234042553192e-06, 'epoch': 0.01}
{'eval_loss': 4.547889232635498, 'eval_runtime': 1.1601, 'eval_samples_per_second': 4.31, 'eval_steps_per_second': 0.862, 'epoch': 0.01}
{'loss': 4.6642, 'learning_rate': 5.319148936170213e-06, 'epoch': 0.02}
{'eval_loss': 4.5045366287231445, 'eval_runtime': 1.156, 'eval_samples_per_second': 4.325, 'eval_steps_per_second': 0.865, 'epoch': 0.02}
{'loss': 4.3982, 'learning_rate': 5.851063829787235e-06, 'epoch': 0.02}
{'eval_loss': 4.456203460693359, 'eval_runtime': 1.0679, 'eval_samples_per_second': 4.682, 'eval_steps_per_second': 0.936, 'epoch': 0.02}
{'loss': 4.8461, 'learning_rate': 6.382978723404256e-06, 'epoch': 0.02}
{'eval_loss': 4.401112079620361, 'eval_runtime': 1.1031, 'eval_samples_per_second': 4.533, 'eval_steps_per_second': 0.907, 'epoch': 0.02}
{'loss': 4.7217, 'learning_rate': 6.914893617021278e-06, 'epoch': 0.02}
{'eval_loss': 4.335606098175049, 'eval_runtime': 1.1135, 'eval_samples_per_second': 4.49, 'eval_steps_per_second': 0.898, 'epoch': 0.02}