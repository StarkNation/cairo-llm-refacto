
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/pechaut/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/datasets/load.py:2097: FutureWarning: 'ignore_verifications' was deprecated in favor of 'verification_mode' in version 2.9.1 and will be removed in 3.0.0.
You can remove this warning by passing 'verification_mode=no_checks' instead.
  warnings.warn(
/home/pechaut/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:221: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/pechaut/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 4.9331, 'learning_rate': 1.8148820326678768e-07, 'epoch': 0.0}
{'eval_loss': 4.6528120040893555, 'eval_runtime': 0.9538, 'eval_samples_per_second': 5.242, 'eval_steps_per_second': 1.048, 'epoch': 0.0}
{'loss': 4.8955, 'learning_rate': 3.6297640653357535e-07, 'epoch': 0.0}
{'eval_loss': 4.652194023132324, 'eval_runtime': 0.956, 'eval_samples_per_second': 5.23, 'eval_steps_per_second': 1.046, 'epoch': 0.0}
{'loss': 5.4687, 'learning_rate': 5.44464609800363e-07, 'epoch': 0.0}
{'eval_loss': 4.651016712188721, 'eval_runtime': 0.9661, 'eval_samples_per_second': 5.175, 'eval_steps_per_second': 1.035, 'epoch': 0.0}
{'loss': 5.3501, 'learning_rate': 7.259528130671507e-07, 'epoch': 0.01}
{'eval_loss': 4.6493730545043945, 'eval_runtime': 0.9707, 'eval_samples_per_second': 5.151, 'eval_steps_per_second': 1.03, 'epoch': 0.01}
{'loss': 4.1946, 'learning_rate': 9.074410163339384e-07, 'epoch': 0.01}
{'eval_loss': 4.646888732910156, 'eval_runtime': 0.9845, 'eval_samples_per_second': 5.078, 'eval_steps_per_second': 1.016, 'epoch': 0.01}
{'loss': 5.5863, 'learning_rate': 1.088929219600726e-06, 'epoch': 0.01}
{'eval_loss': 4.6438984870910645, 'eval_runtime': 1.0967, 'eval_samples_per_second': 4.559, 'eval_steps_per_second': 0.912, 'epoch': 0.01}
{'loss': 5.6041, 'learning_rate': 1.2704174228675136e-06, 'epoch': 0.01}
{'eval_loss': 4.640625953674316, 'eval_runtime': 1.0959, 'eval_samples_per_second': 4.563, 'eval_steps_per_second': 0.913, 'epoch': 0.01}
{'loss': 5.2445, 'learning_rate': 1.4519056261343014e-06, 'epoch': 0.01}
{'eval_loss': 4.636277675628662, 'eval_runtime': 1.0596, 'eval_samples_per_second': 4.719, 'eval_steps_per_second': 0.944, 'epoch': 0.01}
{'loss': 4.7634, 'learning_rate': 1.633393829401089e-06, 'epoch': 0.01}
