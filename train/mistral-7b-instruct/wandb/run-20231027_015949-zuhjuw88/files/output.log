
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/pechaut/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/datasets/load.py:2097: FutureWarning: 'ignore_verifications' was deprecated in favor of 'verification_mode' in version 2.9.1 and will be removed in 3.0.0.
You can remove this warning by passing 'verification_mode=no_checks' instead.
  warnings.warn(
/home/pechaut/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:221: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/pechaut/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 4.9331, 'learning_rate': 1.8148820326678768e-07, 'epoch': 0.0}
{'eval_loss': 4.6528167724609375, 'eval_runtime': 0.9482, 'eval_samples_per_second': 5.273, 'eval_steps_per_second': 1.055, 'epoch': 0.0}
{'loss': 4.8955, 'learning_rate': 3.6297640653357535e-07, 'epoch': 0.0}
{'eval_loss': 4.6522088050842285, 'eval_runtime': 0.9601, 'eval_samples_per_second': 5.208, 'eval_steps_per_second': 1.042, 'epoch': 0.0}
{'loss': 5.4686, 'learning_rate': 5.44464609800363e-07, 'epoch': 0.0}
{'eval_loss': 4.6510491371154785, 'eval_runtime': 0.9678, 'eval_samples_per_second': 5.166, 'eval_steps_per_second': 1.033, 'epoch': 0.0}
{'loss': 5.3499, 'learning_rate': 7.259528130671507e-07, 'epoch': 0.01}
{'eval_loss': 4.649441242218018, 'eval_runtime': 0.9746, 'eval_samples_per_second': 5.13, 'eval_steps_per_second': 1.026, 'epoch': 0.01}
{'loss': 4.1945, 'learning_rate': 9.074410163339384e-07, 'epoch': 0.01}
