{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If in VSCODE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "\n",
    "# Fine-tuned model name\n",
    "new_model = \"StarkWizard/llama-2-7b-cairo-trained-PEFT\"\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "hub_name = \"StarkWizard/codellama-cairo-instruct\"\n",
    "\n",
    "max_steps = 700 # to tweak to get the best out of the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "running this cell is mandatory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Push Model To Hub\n",
    "\n",
    "- Make sure to reinit the kernel\n",
    "- We reload the base model\n",
    "- Load the Peft adapter\n",
    "- Push the merged model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE:\n",
    "\n",
    "We use CPU because of the error: Cannot merge LORA layers when the model is loaded in 8-bit mode\n",
    "Merging is fast, so it's not an issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f39d8b0094c4223a52f3579c44eab0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43c3ea59d6114f639cbaeb24f54990c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)/adapter_config.json:   0%|          | 0.00/532 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea7d11ff6b240da8e88745347569121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading adapter_model.bin:   0%|          | 0.00/80.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c83df4110a4c68be8691430520db4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00013-of-00014.bin:   0%|          | 0.00/967M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af56974ba41944c1bd1adc54863645f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00012-of-00014.bin:   0%|          | 0.00/967M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8154308f1a6b465e98e57e9836fe83fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00014-of-00014.bin:   0%|          | 0.00/847M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee5ec90620b54543a4651213a8509f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00011-of-00014.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d6c5081b8db401198da6fef8813be5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 14 LFS files:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6ebcd9612794e68ac74a5f6eb945443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00010-of-00014.bin:   0%|          | 0.00/944M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09a8b2547f9c4d24aca726660a58389a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00009-of-00014.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0080b89e28a0489793853a2385f9d3aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00008-of-00014.bin:   0%|          | 0.00/967M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fce7da507d54f8db172601c21c0e3b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00007-of-00014.bin:   0%|          | 0.00/967M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03a11dafd83b489bbd8fef54feb84db3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00006-of-00014.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be0642b994384a1595a2e4d873687e2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00005-of-00014.bin:   0%|          | 0.00/944M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22659d442f964e9e8c82fb0143b70266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00004-of-00014.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1d9d5cd6fac4f9aa82cb730d25d0045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00003-of-00014.bin:   0%|          | 0.00/967M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1e056dd413947c68483d04846ec8cb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00014.bin:   0%|          | 0.00/967M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "738f9e3c7b2a4f8fbf2d767c1a30d3d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00014.bin:   0%|          | 0.00/982M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/StarkWizard/codellama-cairo-instruct/commit/1c67da84958a39d50d4a54de21cdb9a8a8e631e8', commit_message='Upload LlamaForCausalLM', commit_description='', oid='1c67da84958a39d50d4a54de21cdb9a8a8e631e8', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "from transformers import  AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_name,\n",
    "                                             trust_remote_code=True,\n",
    "                                             low_cpu_mem_usage=True,\n",
    "                                             device_map={\"\": \"cpu\"},\n",
    "                                             torch_dtype=torch.float16\n",
    "                                             )\n",
    "\n",
    "model_to_merge  = PeftModel.from_pretrained(model, new_model,\n",
    "                        torch_dtype=torch.float16, \n",
    "                        device_map={\"\": \"cpu\"}\n",
    "                         )\n",
    "merged_model = model_to_merge.merge_and_unload()\n",
    "#model.save_pretrained(\"cairo-mistral\")\n",
    "merged_model.push_to_hub(hub_name,max_shard_size=\"1GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
