{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "\n",
    "\n",
    "# Fine-tuned model name\n",
    "new_model = \"StarkWizard/Mistral-7b-instruct-cairo-PEFT\"\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "hub_name = \"StarkWizard/Mistral-7b-instruct-cairo-instruct\"\n",
    "\n",
    "max_steps = 1000 # to tweak to get the best out of the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Push Model To Hub\n",
    "\n",
    "- Make sure to reinit the kernel\n",
    "- We reload the base model\n",
    "- Load the Peft adapter\n",
    "- Push the merged model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE:\n",
    "\n",
    "We use CPU because of the error: Cannot merge LORA layers when the model is loaded in 8-bit mode\n",
    "Merging is fast, so it's not an issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07676b5af75845f89d0d12151e5a038c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "from transformers import  AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_name,\n",
    "                                             trust_remote_code=True,\n",
    "                                             low_cpu_mem_usage=True,\n",
    "                                             device_map={\"\": \"cpu\"},\n",
    "                                             \n",
    "                                             )\n",
    "\n",
    "model_to_merge  = PeftModel.from_pretrained(model, new_model,\n",
    "                        torch_dtype=torch.float16, \n",
    "                        device_map={\"\": \"cpu\"}\n",
    "                         )\n",
    "merged_model = model_to_merge.merge_and_unload()\n",
    "#model.save_pretrained(\"cairo-mistral\")\n",
    "merged_model.push_to_hub(hub_name,max_shard_size=\"1GB\")\n",
    "tokenizer.push_to_hub(hub_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick inference test to check the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you unload reload the kernel before restarting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model from hub for inference\n",
    "\n",
    "- If you just need inference, run this\n",
    "- we load the model from HFace Hub in 4 bits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bb7da059fa14fe0a1e1551bf6512514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/622 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f48ee0c67ee24ea38499fc08ac3d25b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "336d0c5462de4132889dce4d7da73f77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19e3d075cf704503894274aa7dca4c77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00033.bin:   0%|          | 0.00/927M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee403dce96724138a80e4561d81aecb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00033.bin:   0%|          | 0.00/872M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a3fd63052a2484b9f298560cfbde5d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00003-of-00033.bin:   0%|          | 0.00/872M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e252356145a8438ba5037955d1638198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00004-of-00033.bin:   0%|          | 0.00/872M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65475d66c38d4bebb1dd12d89a3dbcc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00005-of-00033.bin:   0%|          | 0.00/872M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a0232bf5a94f198d78db31c92fad48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00006-of-00033.bin:   0%|          | 0.00/872M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa84c10f844540168630a0ff1d66fb8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00007-of-00033.bin:   0%|          | 0.00/872M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "834d7daec7c04966a0371502f80fb7c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00008-of-00033.bin:   0%|          | 0.00/872M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f82c1f2c79334201aa54dab02128e52e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00009-of-00033.bin:   0%|          | 0.00/872M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c07bb8b8e4da44c6a6278a33a26a592c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00010-of-00033.bin:   0%|          | 0.00/872M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a09b7b3a1b1427dac61bb56e248ae02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00011-of-00033.bin:   0%|          | 0.00/872M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0673f7df6af4a73a17e4782b06853a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00012-of-00033.bin:   0%|          | 0.00/872M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a80e66f7254a9db40fb37a650700d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00013-of-00033.bin:   0%|          | 0.00/872M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "065dc5163f0e422fb5fe40cc3ec60d52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00014-of-00033.bin:   0%|          | 0.00/872M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e696859fe54142298ed336db7f648ce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00015-of-00033.bin:   0%|          | 0.00/872M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a5d71db1b5940e3ba2b7cb4d222cb09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00016-of-00033.bin:   0%|          | 0.00/872M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caf27414711949b89c583da0b4fae835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00017-of-00033.bin:   0%|          | 0.00/872M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e8dd04ce26d437c8655c7451f5a4916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00018-of-00033.bin:   0%|          | 0.00/872M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3930c43d796f428e82adb4bb83e51043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00019-of-00033.bin:   0%|          | 0.00/872M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc68f409db304c04a51c6ac5c026a2fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00020-of-00033.bin:   0%|          | 0.00/872M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7905d4cf1494bd88aebec9ee0cd118f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00021-of-00033.bin:   0%|          | 0.00/872M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc0c695cc9e435f9c90a3b2a7a41bbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00022-of-00033.bin:   0%|          | 0.00/872M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e88c29cbd68b4bf1b69ac10376f647de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00023-of-00033.bin:   0%|          | 0.00/872M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c84941e817a84a419527dceaa4ff6da8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00024-of-00033.bin:   0%|          | 0.00/872M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae8eddabe0eb4fefa9e253d981868791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00025-of-00033.bin:   0%|          | 0.00/872M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8381ed177a224f9dab2d310aea426240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00026-of-00033.bin:   0%|          | 0.00/872M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7424bc3dad0841ccbabeb8428c6ac622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00027-of-00033.bin:   0%|          | 0.00/872M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fcc8b263f274d96a51ac6feb03d0ee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00028-of-00033.bin:   0%|          | 0.00/872M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e32c3a735cc54c8db9d935ea23b1b8fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00029-of-00033.bin:   0%|          | 0.00/872M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1824b9446f894ef5a1e5d0d7cde743b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00030-of-00033.bin:   0%|          | 0.00/872M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "681ffbab053a4c91a11ec7a741b6e537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00031-of-00033.bin:   0%|          | 0.00/872M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eeb008829a9462688d7f52d011594f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00032-of-00033.bin:   0%|          | 0.00/872M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beeea2481faf4d649a7821640e2592f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00033-of-00033.bin:   0%|          | 0.00/994M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "743f699c53484f1e9f9c833b72a11253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Attention Sinks] Injected Position Shifting into 32 attention classes.\n",
      "[Attention Sinks] Injected Attention Sink KV Cache into 1 model class.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, TextStreamer, GenerationConfig, BitsAndBytesConfig\n",
    "from attention_sinks import AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=hub_name,\n",
    "                                             trust_remote_code=True,\n",
    "                                             device_map={\"\": 0},\n",
    "                                             attention_sink_size=4,\n",
    "                                             quantization_config=bnb_config,\n",
    "                                            attention_sink_window_size=252, # <- Low for the sake of faster generation\n",
    "                                             )\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Votre GPU supporte bfloat16!\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST]\n",
      "<<SYS>>\n",
      "A student asks you a question about Cairo 1. Provide a concise answer to the student's questions,do not expand the subject of the question, do not introduce any new topics or new question not provided by the student.\n",
      "Make sure the explanations never be longer than 300 words.Don’t justify your answers. Don’t give information not mentioned in the CONTEXT INFORMATION.provide only one solution <SYS>>\n",
      "\n",
      "Question: I'm working in Cairo 1 : What is the naming convention for enum variants, give some examples "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pechaut/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/pechaut/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[/INST]\n",
      "* The naming convention for enum variants follows PascalCase, where the first letter of each word is capitalized and underscores separate words. Examples include 'Option::Some(value)', 'Message::Quit', 'ShapeGeometry::Rectangle'. * If an enum variant name includes multiple words, it should be spelled with PascalCase, as shown with Message::Quit.\n",
      "\n",
      "[/INST]\n",
      "* If you are using Cairo and want to learn more about its features and capabilities, please refer to this book which provides comprehensive guidance on Cairo's core libraries and tooling:\n",
      "\n",
      "<a href=\"https://rustbyexample.com/book-chapter-07-ownable-component#:~:text=This%20chapter%20will%20give%20an%20overview%20of%20the%20core%20library.\" target=\"_blank\">Learn more about Cairo's core library and tooling!</a>\n",
      "\n",
      "* For more details about the ownership mechanism in Cairo, please check out this resource:\n",
      "\n",
      "<a href=\"https://www.reddit.com/r/Cairolang/comments/8xvj45/cairo_language_and_its_ownership_mechanism/\" target=\"_blank\">Explore the ownership system in Cairo!</a>\n",
      "\n",
      "* You can use this tool named Scarb to compile your Cairo programs: <a href=\"https://docs.scarb.org/en/scarb/index.html\" target=\"_blank\">Scarb Documentation - scarb.org</a>. Please note that this example does not work with Scarb yet due to how the compiler handles modules. However, the code still makes sense conceptually.\n",
      "\n",
      "* To use Scarb with your Cairo program, follow these steps:\n",
      "\t+ Install Scarb: `scarb install`\n",
      "\t+ Compile your Cairo program with Scarb: `scarb build --target wasm`\n",
      "\t+ Run the compiled code (starknet signer): `./target/deployer/wasm_zk_STARK_contract.json`\n",
      "\n",
      "* As of StarkNet v2.3.0, there is no support for private functions in contracts because they would require passing the contract state to the function. This is because private functions have access to the contract's storage but public functions don't. Private functions would need to mutate the contract's storage to change its state, but public functions cannot modify the contract's state, per StarkNet's rules. In the future, when StarkNet supports private functions inside contracts, we will revisit this issue and update this document accordingly.\n",
      "* From StarkNet's documentation:\n",
      "The current version of StarkNet doesn't support private functions within contracts because they would require passing the contract state to the function. This is because private functions have access to the contract's storage but public functions don't. Private functions would need to mutate the contract's storage to change its state, but public functions cannot modify the contract's state, per StarkNet's rules. In the future, when StarkNet supports private functions inside contracts, we will revisit this issue and update this document accordingly.\n",
      "\n",
      "* For contract developers who want to create private functions but currently face limitations, consider writing logic outside of a contract in a component instead. This approach can also be useful when implementing complex logic that might benefit from being modularized and separated from the contract's main execution flow. Component logic "
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer, GenerationConfig\n",
    "\n",
    "\n",
    "#prompt = \"Create an array and append 3 domestic animal names as string\"\n",
    "#prompt = \"give an exemple of constructor\"\n",
    "#prompt = \"create a felt and affect it a v/alue of 1\"\n",
    "#prompt=\"In the example 'let y = { let x = 3; x + 1 };', what is the value of y? give a similar sample\"\n",
    "#prompt=\"How can you print a variable's value in Cairo? Give a full sample using an array\"\n",
    "#prompt = \"hat type of operations can be considered expressions in Cairo? Give an exemple.\"\n",
    "#prompt = \"What is a characteristic feature of Felt252Dict<T> when interacting with it? Give a sample\"\n",
    "#prompt = \"what are spans used for\"\n",
    "#prompt = \"How do I know if an array is empty, give 2 examples\"\n",
    "#prompt = \"what makes Cairo special\"\n",
    "#prompt = \"Create an array and append some domestic animal names\"\n",
    "#prompt=\"write a contract that computes the fibonacci of caller's address and explain the weakness of the program\"\n",
    "#prompt=\"write a contract that returns the fibonacci of the caller address. Provide the steps to compile and deploy\"\n",
    "#prompt=\"write an empty contract template and give the instructions to compile and deploy it\"\n",
    "#prompt=\"write a contract that stores the caller's address in its constructor and a function that returns the fibonnacci of that address\"\n",
    "#prompt=\"How can you implement the `Copy` trait on a custom type in Cairo?\"\n",
    "#prompt = \"What's the significance of the `self` parameter in my method of the `CubeGeom` trait?\"\n",
    "#prompt = \"write a contract with a function that returns the address of the caller\"\n",
    "prompt=\"how can i cast a variable into another ?\"\n",
    "\n",
    "prompt = \"What do the terms 'parent' and 'child' mean in the context of modules?\"\n",
    "prompt=\"What can be defined in trait and impl blocks related to a type?\"\n",
    "prompt = \"How do I know if an array is empty, give 2 examples\"\n",
    "\n",
    "prompt=\"write a  basic simple contract that returns the fibonacci of a number\"\n",
    "prompt = \"Create an array and append 3 domestic animal names as string\"\n",
    "\n",
    "\n",
    "\n",
    "#prompt = \"How do I know if an array is empty, give 2 examples \"\n",
    "\n",
    "#prompt = \"Provide a basic code snippet that defines a struct named 'User' with fields like 'active', 'username', and so on\"\n",
    "\n",
    "\n",
    "#prompt=\"write an empty contract template and give the instructions to compile and deploy it\"\n",
    "\n",
    "#prompt = \"write a contract with a function that returns the address of the caller\"\n",
    "#prompt = \"write a contract with a add_felt function that gets the sum of 2 felt parameters\"\n",
    "#prompt = \"Create an array and append 3 domestic animal names as string\"\n",
    "prompt = \"write a contract that returns the fibonacci of the caller address\"\n",
    "\n",
    "prompt=\"Can you provide a code snippet that declares an external function which can modify the contract's state?\"\n",
    "prompt = \"What's the difference between Nullable<T> and Option in terms of where the wrapped value is stored?\"\n",
    "\n",
    "prompt = \"How does the Cairo convention style name functions and variables?\"\n",
    "\n",
    "prompt = \"write a contract that returns the fibonacci of the caller address\"\n",
    "\n",
    "prompt = \"What does the #[starknet::component] attribute signify?\"\n",
    "\n",
    "prompt = \" What is the naming convention for enum variants, give some examples\"\n",
    "\n",
    "text =f\"\"\"[INST]\n",
    "<<SYS>>\n",
    "A student asks you a question about Cairo 1. Provide a concise answer to the student's questions,do not expand the subject of the question, do not introduce any new topics or new question not provided by the student.\n",
    "Make sure the explanations never be longer than 300 words.Don’t justify your answers. Don’t give information not mentioned in the CONTEXT INFORMATION.provide only one solution <SYS>>\n",
    "\n",
    "Question: I'm working in Cairo 1 :{prompt} \n",
    "[/INST]\"\"\"\n",
    "\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    streamer = TextStreamer(tokenizer)\n",
    "    generated_tokens = model.generate(\n",
    "        input_ids,\n",
    "        generation_config=GenerationConfig(\n",
    "            # use_cache=True is required, the rest can be changed up.\n",
    "            use_cache=True,\n",
    "            min_new_tokens=1,\n",
    "            max_new_tokens=1050,\n",
    "\n",
    "                                early_stopping=True,\n",
    "                                bos_token_id=model.config.bos_token_id,\n",
    "                                eos_token_id=model.config.eos_token_id,\n",
    "                                pad_token_id=model.config.eos_token_id,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15,\n",
    "                               do_sample=True\n",
    "        ),\n",
    "        streamer=streamer,\n",
    "    )\n",
    "    # Decode the final generated text\n",
    "    output_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST]\n",
      "<<SYS>>\n",
      "provide only one solution and no other possible solution, stick to the main topic, do not introduce any new topics or new question not provided by the student.\n",
      "Be conc.Don’t justify your answers. <SYS>>\n",
      "\n",
      "Question: I'm working in Cairo 1 :write a  contract that returns the fibonacci of the caller address \n",
      "[/INST]\n",
      "```\n",
      "   #[starknet::contract]\n",
      "   mod Fib {\n",
      "       use starknet::get_caller_address;\n",
      "\n",
      "       fn fib(self: @Fib) -> (u256, u256) {\n",
      "           let mut a = 0;\n",
      "           let mut b = 1;\n",
      "           if get_caller_address() == 0x0 {\n",
      "               return (a, b);\n",
      "           } else {\n",
      "               let temp = (b, a + b);\n",
      "               (temp.0, temp.1)\n",
      "           }\n",
      "       }\n",
      "   }\n",
      "```\n",
      "\n",
      " This is a simple example of how you can write a function that returns the Fibonacci number for the caller based on their address. If the caller address is 0x0 (the contract's own address), it returns the numbers 0 and 1 as they are both 0 and 1, respectively. Otherwise, it computes the Fibonacci recursively using the caller's address as the base case.\n",
      "\n",
      " To call this function from outside the contract, we would need to wrap it inside another function with the appropriate parameters and then call that wrapped function. For instance, we could define a public view function like this:\n",
      "\n",
      "   fn view_fib(self: @ContractState) -> (u24, u24) {\n",
      "       self.fib.view(self.address)\n",
      "   }\n",
      "\n",
      "   Then we could call this view function externally by passing in the contract's address as a parameter. The contract would then call its internal 'fib' function with its own address as the base case, returning the Fibonacci numbers for that address.\n",
      "\n",
      "   It's important to note that when calling functions from within a contract, we don't need to pass in the contract's state or the calldata type, as the execution context remains that of the called contract. However, when calling functions from outside the contract, the execution context becomes that of the calling contract, so we need to pass in the necessary parameters and types explicitly.\n",
      "\n",
      "   In the context of Starknet contracts, the execution context is determined by the caller's address, which is also known as the contract address. When a contract is executed, the execution context is that of the caller who invoked the contract. This means that when a contract is passed as a parameter to another function, the execution context of the inner function will be that of the caller of the outer function.\n",
      "\n",
      "   For example, if a contract A calls a contract B, the execution context of B will be that of A, not that of B. This allows us to pass control to different parts of our code without having to re-declare variables or states.\n",
      "\n",
      "   When writing Starknet contracts, it's important to keep track of the execution context and ensure that functions are called with the correct parameters and types, especially when passing contracts as arguments to other functions. This helps avoid confusion and ensures that the logic of the program behaves as intended.\n",
      "\n",
      "   In summary, understanding the execution context and its implications is crucial when designing and implementing Starknet contracts, as it affects how functions are called and passed between contracts, and ultimately determines the behavior of the program. By being aware of these concepts, developers can write more efficient and secure code for Starknet applications.</s>\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer, GenerationConfig\n",
    "\n",
    "\n",
    "prompt = \"Create an array and append domestic animal names\"\n",
    "#prompt = \"give an exemple of constructor\"\n",
    "#prompt = \"create a felt and affect it a value of 1\"\n",
    "prompt=\"In the example 'let y = { let x = 3; x + 1 };', what is the value of y? give a similar sample\"\n",
    "#prompt=\"How can you print a variable's value in Cairo? Give a full sample using an array\"\n",
    "#prompt = \"hat type of operations can be considered expressions in Cairo? Give an exemple.\"\n",
    "#prompt = \"What is a characteristic feature of Felt252Dict<T> when interacting with it? Give a sample\"\n",
    "#prompt = \"How do I create a span ? Give a sample\"\n",
    "#prompt = \"How do I know if an array is empty, give 2 examples\"\n",
    "#prompt = \"what makes Cairo special\"\n",
    "#prompt = \"Create an array and append some domestic animal names\"\n",
    "#prompt=\"write a contract that computes the fibonacci of caller's address and explain the weakness of the program\"\n",
    "#prompt=\"write a contract that returns the fibonacci of the caller address. Provide the steps to compile and deploy\"\n",
    "#prompt=\"write an empty contract template and give the instructions to compile and deploy it\"\n",
    "#prompt=\"write a contract that stores the caller's address in its constructor and a function that returns the fibonnacci of that address\"\n",
    "#prompt=\"How can you implement the `Copy` trait on a custom type in Cairo?\"\n",
    "#prompt = \"What's the significance of the `self` parameter in my method of the `CubeGeom` trait?\"\n",
    "#prompt = \"write a contract with a function that returns the address of the caller\"\n",
    "#prompt=\"how can i cast a variable into another ?\"\n",
    "\n",
    "#prompt = \"What do the terms 'parent' and 'child' mean in the context of modules?\"\n",
    "#prompt=\"What can be defined in trait and impl blocks related to a type?\"\n",
    "#prompt = \"How do I know if an array is empty, give 2 examples\"\n",
    "\n",
    "#prompt=\"write a  basic simple contract that returns the fibonacci of the caller's address\"\n",
    "prompt = \"Create an array and append some domestic animal names as string\"\n",
    "\n",
    "\n",
    "\n",
    "#prompt = \"How do I know if an array is empty, give 2 examples \"\n",
    "\n",
    "#prompt = \"Provide a basic code snippet that defines a struct named 'User' with fields like 'active', 'username', and so on\"\n",
    "\n",
    "\n",
    "#prompt=\"write an empty contract template and give the instructions to compile and deploy it\"\n",
    "\n",
    "#prompt = \"write a contract with a function that returns the address of the caller\"\n",
    "#prompt = \"write a contract with a add_felt function that gets the sum of 2 felt parameters\"\n",
    "#prompt = \"Create an array and append 3 domestic animal names as string\"\n",
    "#prompt = \"write a contract that returns the fibonacci of the caller address\"\n",
    "\n",
    "#prompt=\"Can you provide a code snippet that declares an external function which can modify the contract's state?\"\n",
    "#prompt = \"What's the difference between Nullable<T> and Option in terms of where the wrapped value is stored?\"\n",
    "\n",
    "\n",
    "#prompt = \"How does the Cairo convention style name functions and variables?\"\n",
    "\n",
    "prompt = \"write a  contract that returns the fibonacci of the caller address\"\n",
    "\n",
    "#prompt = \"What does the #[starknet::component] attribute signify?\"\n",
    "\n",
    "#prompt = \" What is the naming convention for enum variants, give some examples\"\n",
    "\n",
    "text =f\"\"\"[INST]\n",
    "<<SYS>>\n",
    "provide only one solution and no other possible solution, stick to the main topic, do not introduce any new topics or new question not provided by the student.\n",
    "Be conc.Don’t justify your answers. <SYS>>\n",
    "\n",
    "Question: I'm working in Cairo 1 :{prompt} \n",
    "[/INST]\"\"\"\n",
    "\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    streamer = TextStreamer(tokenizer)\n",
    "    generated_tokens = model.generate(\n",
    "        input_ids,\n",
    "        generation_config=GenerationConfig(\n",
    "                       # use_cache=True is required, the rest can be changed up.\n",
    "            use_cache=True,\n",
    "            min_new_tokens=1,\n",
    "            max_new_tokens=1050,\n",
    "            penalty_alpha=0.2,\n",
    "            top_k=2080,\n",
    "            do_sample=True,\n",
    "            top_p=0.96,\n",
    "            no_repeat_ngram_size =6,\n",
    "            repetition_penalty = 1.14,\n",
    "            temperature=0.001,\n",
    "                                bos_token_id=model.config.bos_token_id,\n",
    "                                eos_token_id=model.config.eos_token_id,\n",
    "                                pad_token_id=model.config.eos_token_id,\n",
    "        ),\n",
    "        streamer=streamer,\n",
    "    )\n",
    "    # Decode the final generated text\n",
    "    output_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cairo-code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
